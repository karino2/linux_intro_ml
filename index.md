数学とか論文読んだりPython見様見真似で叩いたりはなんでも無いけどパソコンの使い方とかsshとかで詰まりがちな物理や数学の理論出身の機械学習屋に、データ分析以外のLinux周辺の基礎をまとめたいと思ってこのページを作ってます。

### あらすじ

つくばの山奥で宇宙や素粒子について考えてた人たちが、うっかり山を降りてきて社会に出てしまう現象が多く観測されるようになりました。
そうした人は機械学習屋になる事も比較的多いので、自分とも接点があります。この文書ではそうした人達をターゲットにします。

彼らは本業のデータ分析や機械学習に関しては、しっかりとした土台を持っているからか、
はたまたそれまでの経験と近いからか、
すぐに適応して高いパフォーマンスを発揮する傾向が見られます。うらやましい限りです。

ですが彼らは本業のデータ分析「以外のところ」での社会経験が無く、
社会や会社の組織とかよりも宇宙や極微細な事ばかり考えていたせいか、
ひどいケースだと「土浦は意外と都会だ」などと発言したりと日常業務に支障をきたしたりする事も珍しくありません。

そこまで非常識では無くても、あまりLinuxなどを触って来なかった理論系の人は、
日常的なコマンドやAWS等で作業する上でのsshなどの所でつまづきがちです。
ここではそうした人たちに、データ分析「以外」の基礎知識というかデータ分析をする上で必要となる最低限の事をまとめてみたいと思います。

### 技術的な背景

データ分析は計算量が大量に必要な都合で、割と特殊なクラウドの環境に接続して作業する必要がある事も多くあります。
また、大きなデータを扱う必要がある為、エディタで開けない大きなファイルの処理などをコマンドラインで行ったり、
長い計算の途中経過を眺めるためにtailのfオプションでログについていったり、
といった事を日常的にする必要があります。

そうしたLinuxのコマンドやsshをつなぐトラブルシュートなどは意外と特殊な知識を要求される事がある反面、
要求される項目自体は多くはありません。
Linuxのシステム管理に関わる多くのコマンドはとりあえずは必要では無いし、
ネットワークもとりあえずssh周りにだけ詳しければ良い。
データ分析を仕事にするなら、かならずインフラに詳しい人がチーム内に一人くらいは居るはずです。
彼らと同じ事を知る必要は無くて、彼らとコミュニケートして問題を解決出来れば十分なのです。

ひるがえって世の中のLinux入門などの書籍では、網羅的にコマンドが紹介されがちで、その大部分は必要無い。
しかもそうしたコマンドの外になってしまうssh、docker、git、tmux、Invokeなどの必要な知識は入っていない。
その為、いらない事ばかりを学んで必要な事がいつまでたっても足りない、となりがちに思います。

### 本文書の目的

以上のような前提を踏まえ、本文書では実務に必要な事を一通り列挙するのを目指します。
といってもgitやdockerなど、単体で本になるような分量の事をすべて解説する気は無くて、
必要に応じて外部のドキュメントにアウトソースしていくつもりです。
何をやったらいいか分からない、という人に、とりあえずこれを全部やっていけば良い、という事を示す為に、
必要な事が全て参照されている文書を目指します。

機械学習の環境は各組織で特殊な事が多いのですが、
一般論だけではその特殊な環境に対応するのが難しいと思います。
そこである程度良くある特殊な環境を想定して解説は行い、それとの差分を各自が詳しいチーム内の人に聞けるようになれば良い、と思っています。

また、使う人の好みで選んで良い事に関しても、一番のオススメと思える物をあえて断定して説明していきます。
というのは素人は好みで選ぶというのが難しいからです。
最初は私が良いと思う選択を試してみて、なれてきたらそれ以外の物を触ってみれば良いと思い、と考えています。

全体として、最大公約数では無くある特定の選択を解説していこうと思っています。

### 解説予定の一覧

- [Linuxの基本的な操作やコマンド](linux_cmd.md)
  - cdとかpushdとかfindとかxargsとか
- [環境構築に必要最低限のシェル入門](shell_intro.md)
  - 変数展開とかバッククオートとか
- [テキスト処理](text_op.md)
  - grep, sed, awk (awkは基本だけ)
  - head, tail
  - vim(ターミナルでの作業用の最低限だけ。基本はVSCode推奨という立場）
- [インスタンスのセットアップ周辺（ユーザーとかpermissionもここ）](machine_admin.md)
  - wgetかcurl
  - du, df, mount (ボリュームの追加とか)
  - apt
  - chmodとか
  - sshと.ssh/configとscpとポートフォワード(Jupyter Notebook用）
  - tmux (計算をSIGHUPを無視して走らせ続ける為専用）
- その他のインストールが必要なコマンド
  - agはここに入れる
  - tmux（こちらには詳細を）
  - Invoke (Pythonの。makeの代わり)
- git
- docker

### 想定する3つのシチュエーション

機械学習をやる人が良く遭遇するLinuxのシチュエーションは大きく3つに分けられると思う。

1. ちょっとした調査などの為にローカルで動かす
2. クラウドのインスタンスにDockerを建ててJupyterを動かす
3. 無職なのでcolabで頑張る

この文書を読みながらちょっと動かす場合は1が多いだろうと思う。
一番単純だが、実務としてはあまり使わない環境でもある。

2が普通の実務で遭遇する環境で、2だけ出来ればだいたい良い。
ただ、勉強目的で同じ環境を作るのはちょっと手間。
この場合は普通は踏み台があり、サーバーのインスタンスがあり、その中にdockerのコンテナがある。
それぞれのLinuxを触る必要があるので、それぞれどういう特徴があるかを理解しておく必要がある。

呼び名としては、ローカル、ホスト、コンテナ内、というふうに呼ぶ（ホストはつまりクラウド上のインスタンス）。
踏み台はssh周りの話をする時以外はあまり出てこない予定。

3は無職なら限界までcolabで頑張るのが一番安上がりと思うのだが、
仕事では意外と活かしづらいので無職とか趣味で家で触る時くらいしかなかなか出番が無い。
colabは最初使っている段階では下のLinuxを意識する必要は無いが、たまに何かインストールしないといけない事もちょこちょこあって、
そういう時に下の環境を意識しないといけない事がある。

この文書ではだいたいは1と2について話していく予定。3は普通の環境の知識を身に着けた上で少し触っていれば分かると思うので（逆にその程度の知識しか自分にも無い）

### 昔内輪の勉強会向けに書いた入門

[詳解ディープラーニング勉強会 ページ](https://karino2.github.io/deeplearning-tensorflow-keras-study/)

これをもっとまともにするのを想定している。